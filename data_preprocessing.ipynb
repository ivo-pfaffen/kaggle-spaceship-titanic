{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "The competition provides three files: \n",
    "* `train.csv`: personal records for about two-thirds (~8700) of the passengers, to be used as training data\n",
    "* `test.csv`: personal records for the remaining one-third (~4300) of the passengers, to be used as test data\n",
    "* `sample_submission`: a submission file in the correct format\n",
    "Which are all located in the `data/` directory.\n",
    "\n",
    "Our first task is to load and preprocess the data to be able to feed it into our neural network for training. As we can see, there's lots of non-numerical data.\n",
    "We are going to perform feature encoding for each of the columns containing non-numerical data, and some feature engineering after that to (potentially) improve the model's performance. \n",
    "\n",
    "### First things first\n",
    "Importing libraries. Make sure you have them installed (check the instructions in the `README.md`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from statistics import mean\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Load training and test data\n",
    "base_file_path = \"./data/\"\n",
    "test_data = pd.read_csv(base_file_path+'test.csv')\n",
    "training_data = pd.read_csv(base_file_path+'test.csv')\n",
    "print(training_data.columns)\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning planets into numbers\n",
    "We will turn the `HomePlanet` and `Destination` columns into numeric values representing each planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"Name\" column - we won't be using that data to train the model\n",
    "training_data = training_data.drop('Name', axis=1)\n",
    "\n",
    "# Load the 'HomePlanet' and'\"Destination' columns\n",
    "home_planets = training_data.get('HomePlanet').unique().tolist()[:-1]\n",
    "destinations = training_data.get('Destination').unique().tolist()[:-1]\n",
    "\n",
    "# Convert both columns to a dictionary \n",
    "home_planets_map = {k: v for v, k in enumerate(home_planets)}\n",
    "destinations_map = {k: v for v, k in enumerate(destinations)}\n",
    "\n",
    "# Turn both columns into their respective keys in the dictionary\n",
    "training_data['HomePlanet'] = training_data['HomePlanet'].replace(home_planets_map)\n",
    "training_data['Destination'] = training_data['Destination'].replace(destinations_map)\n",
    "\n",
    "# Mark them as numeric\n",
    "training_data['HomePlanet'] = pd.to_numeric(training_data['HomePlanet'], errors='coerce')\n",
    "training_data['Destination'] = pd.to_numeric(training_data['Destination'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cabin IDs are a problem\n",
    "Cabin IDs are structured as follows: \"letter/number/letter\" (i.e B/0/P).\n",
    "\n",
    "We need to:\n",
    "- encode them\n",
    "- replace missing values in a non-random way\n",
    "\n",
    "The number in the middle seems to be increasing non-monotonically, so I decided to replace missing values for a random number in the following range: $|(\\text{prev valid value} - 5, \\text{prev valid value})|$.\n",
    "\n",
    "For the letters, it's clear that they repeat, so I decided to we create probability mappings and replace missing letters based on those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the 'Cabin' column, whose values are formatted as 'letter/number/letter'\n",
    "cabins = training_data.get('Cabin')\n",
    "\n",
    "# Separate 'Cabin' into 'Cabin_N' with N \\in {1,2,3}\n",
    "pattern = re.compile(r'^([a-zA-Z])/(\\d+)/([a-zA-Z])$')\n",
    "\n",
    "parsed_cabins = [\n",
    "    (matches.group(1), int(matches.group(2)), matches.group(3)) if (matches := pattern.match(str(cabin)))\n",
    "    else ('NA', -1, 'NA')\n",
    "    for cabin in cabins\n",
    "]\n",
    "\n",
    "# Transpose the parsed_cabins to separate lists for cabin_1, cabin_2, and cabin_3\n",
    "cabin_1, cabin_2, cabin_3 = map(list, zip(*parsed_cabins))\n",
    "\n",
    "# Replace missing numbers for a random value in |(previous valid value - 5, previous valid value)| \n",
    "# just because it seems to work\n",
    "prev_valid_val = None\n",
    "cabin_2 = [abs(random.randint(prev_valid_val-10, prev_valid_val)) if x == -1 else (prev_valid_val := x) for x in cabin_2]\n",
    "\n",
    "\n",
    "# Methods to compute probabilities for letter in the cabin IDs\n",
    "def list_to_probability_mapping(value_list):\n",
    "    value_counts = Counter(value_list)\n",
    "    total_count = len(value_list)\n",
    "\n",
    "    probability_mapping = {value: count / total_count for value, count in value_counts.items()}\n",
    "\n",
    "    return probability_mapping\n",
    "\n",
    "def get_index_from_probability_mapping(probability_mapping):\n",
    "    indices = list(range(len(probability_mapping)))\n",
    "    probabilities = list(probability_mapping.values())\n",
    "    return np.random.choice(indices, p=probabilities)\n",
    "\n",
    "def get_mean_val_randomized(dictionary):\n",
    "  return int(np.random.normal(loc=mean(dictionary), scale=2))\n",
    "\n",
    "# Create probability mappings\n",
    "cabin_1_map = list_to_probability_mapping(cabin_1)\n",
    "cabin_3_map = list_to_probability_mapping(cabin_3)\n",
    "\n",
    "# Replace missing letters for another value, weighing probability\n",
    "cabin_1 = [get_index_from_probability_mapping(cabin_1_map) if x == 'NA' else x for x in cabin_1]\n",
    "cabin_3 = [get_index_from_probability_mapping(cabin_3_map) if x == 'NA' else x for x in cabin_3]\n",
    "\n",
    "# Replace the 'Cabin' column for 'Cabin_ID_1, Cabin_ID_2, Cabin_ID_3' with their respective values\n",
    "training_data = training_data.drop('Cabin', axis=1)\n",
    "training_data['Cabin_ID_1'] = cabin_1\n",
    "training_data['Cabin_ID_2'] = cabin_2\n",
    "training_data['Cabin_ID_3'] = cabin_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning booleans into numbers is easier\n",
    "The \"CryoSleep\", \"VIP\" and \"Transported\" columns can be turned into numbers trivially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['CryoSleep'] = np.where(training_data['CryoSleep'] == True, 1, 0)\n",
    "training_data['VIP'] = np.where(training_data['VIP'] == True, 1, 0)\n",
    "training_data['Transported'] = np.where(training_data['Transported'] == True, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are CabinIDs, tho?\n",
    "We plot a heatmap to visualize the value distribution and correlation between the Cabin_ID_1 and Cabin_ID_3 values. We also check the cardinality of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a count of each combination\n",
    "counts = training_data.groupby(['Cabin_ID_1', 'Cabin_ID_3']).size().unstack(fill_value=0)\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(counts, annot=True, cmap=\"YlGnBu\", fmt='d')\n",
    "plt.title('Heatmap of Non-Numerical Values')\n",
    "plt.xlabel('Cabin_ID_3')\n",
    "plt.ylabel('Cabin_ID_1')\n",
    "plt.show()\n",
    "\n",
    "# Check cardinality\n",
    "print(training_data['Cabin_ID_1'].nunique(), training_data['Cabin_ID_3'].nunique())\n",
    "print(training_data['Cabin_ID_1'].unique(), training_data['Cabin_ID_3'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some interesting results, so I'll keep the plot here.\n",
    "\n",
    "Given the cardinalities are low (and I want to train the NN and check accuracy), \n",
    "I'll just replace each category with a small integer, and improve on it later if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn Cabin_ID_1, Cabin_ID_3 into their respective labels\n",
    "training_data['Cabin_ID_1'] = training_data['Cabin_ID_1'].astype('category').cat.codes\n",
    "training_data['Cabin_ID_3'] = training_data['Cabin_ID_3'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, turning Passenger IDs into numbers\n",
    "I see that there might be a correlation between Passenger IDs and Cabin IDs (which would make sense if, say, people booked together and were placed in the same cabins). I want to check this, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract number before '__' from first column\n",
    "training_data['deck_num'] = training_data['PassengerId'].astype(str).str.extract(r'^(\\d+)').astype(float)\n",
    "\n",
    "# Concatenate all three columns into a single string\n",
    "training_data['cabin_combo'] = training_data[['Cabin_ID_1', 'Cabin_ID_2', 'Cabin_ID_3']].astype(str).agg('-'.join, axis=1)\n",
    "\n",
    "# Correlation\n",
    "is_unique = training_data.groupby('cabin_combo')['deck_num'].nunique() == 1\n",
    "print(f\"{is_unique.mean()*100:.2f}% of combos map to exactly one deck_num\")\n",
    "\n",
    "training_data = training_data.drop(['deck_num','cabin_combo'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that CabinIDs are almost entirely determined by PassengerIDs which share the suffix before '__'.\n",
    "I'll just split them into `GroupId`, and then a \"within-group\" index (as `GroupRank`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split PassengerIDs into groups and \"within-group\" ranks \n",
    "training_data['GroupId']   = training_data['PassengerId'].str.split('_').str[0]\n",
    "training_data['GroupRank'] = training_data['PassengerId'].str.split('_').str[1].astype(int)\n",
    "\n",
    "# Derive a \"group size\" feature instead of keeping an arbitrary ID\n",
    "training_data['GroupSize'] = training_data.groupby('GroupId')['GroupId'].transform('count')\n",
    "\n",
    "# Derive a \"traveling alone\" feature\n",
    "training_data['IsAlone'] = (training_data['GroupSize'] == 1).astype(int)\n",
    "\n",
    "# Drop the original PassengerId column and the GroupId\n",
    "training_data = training_data.drop(['PassengerId','GroupId'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done! Let's save the processed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spending_categories  = ['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']\n",
    "\n",
    "for spending_cat in spending_categories:\n",
    "    s = training_data[spending_cat]\n",
    "    is_zero = s == 0\n",
    "    cat = pd.Series(index=s.index, dtype=\"Int64\")\n",
    "    cat[is_zero] = 0\n",
    "    nonzero = s[~is_zero]\n",
    "    cat[~is_zero] = pd.qcut(nonzero, q=3, labels=[1,2,3], duplicates='drop')\n",
    "    training_data[spending_cat+'_Cat'] = cat\n",
    "\n",
    "\n",
    "sns.countplot(data=training_data, x='FoodCourt_Cat', order=[0,1,2,3])\n",
    "plt.show()\n",
    "\n",
    "# Drop the original spending categories\n",
    "training_data = training_data.drop(spending_categories,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaNs...\n",
    "I see that there are lots of missing values. Let's see where exactly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaNs per column:\")\n",
    "print(training_data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just because I'm tired of fixing the data and really want to train the model right now, I'll just replace NaNs for their respective averages in each column :p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = training_data.mean(numeric_only=True).round().astype('Int64')\n",
    "training_data = training_data.fillna(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaNs per column:\")\n",
    "print(training_data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No more invald values! Great. Let's scale the columns, reorganize them, and export the final training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before that, let's group essential and non-essential categories together\n",
    "def avg_nonzero(cols):\n",
    "    def compute(row):\n",
    "        values = [row[col] for col in cols if row[col] > 0]\n",
    "        return sum(values) / len(values) if values else 0\n",
    "    return compute\n",
    "\n",
    "essential = [\"FoodCourt_Cat\"]\n",
    "non_essential = [\"RoomService_Cat\", \"ShoppingMall_Cat\", \"Spa_Cat\", \"VRDeck_Cat\"]\n",
    "\n",
    "training_data[\"EssentialSpendingCat\"] = training_data.apply(avg_nonzero(essential), axis=1).round().astype(int)\n",
    "training_data[\"NonEssentialSpendingCat\"] = training_data.apply(avg_nonzero(non_essential), axis=1).round().astype(int)\n",
    "\n",
    "# training_data.drop(essential+non_essential, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the \"Transported\" column to the end of the df\n",
    "col = training_data.pop(\"Transported\")\n",
    "training_data[\"Transported\"] = col\n",
    "\n",
    "# Save processed training data in a new CSV\n",
    "training_data.to_csv(base_file_path+'train_processed.csv', index=False)\n",
    "\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- ~~Replacing cabin_1, cabin_3 for numerical data~~\n",
    "- ~~Turning Passenger IDs into numbers~~\n",
    "- Turn Cabin IDs into more relevant stuff, do some feature eng\n",
    "- ~~Turn RoomService,FoodCourt,ShoppingMall,Spa,VRDeck into spending categories (maybe with percentiles?) instead of numbers~~ \n",
    "- Group non-essential and essential spending categories together"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
