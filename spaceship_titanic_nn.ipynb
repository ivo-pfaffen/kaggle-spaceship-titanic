{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "The competition provides three files: \n",
    "* `train.csv`: personal records for about two-thirds (~8700) of the passengers, to be used as training data\n",
    "* `test.csv`: personal records for the remaining one-third (~4300) of the passengers, to be used as test data\n",
    "* `sample_submission`: a submission file in the correct format\n",
    "Which are all located in the `data/` directory.\n",
    "\n",
    "Our first task is to load and preprocess the data to be able to feed it into our neural network for training. As we can see, there are lots of non-numeric data.\n",
    "We are going to perform feature encoding for each of the columns containing non-numerical data, and some feature engineering after that to (potentially) improve the model's performance. \n",
    "\n",
    "### First things first\n",
    "Importing libraries. Make sure you have them installed (check the instructions in the `README.md`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from statistics import mean\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Load training and test data\n",
    "base_file_path = \"./data/\"\n",
    "test_data = pd.read_csv(base_file_path+'test.csv')\n",
    "training_data = pd.read_csv(base_file_path+'train.csv')\n",
    "print(training_data.columns)\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning planets into numbers\n",
    "We will turn the `HomePlanet` and `Destination` columns into numeric values representing each planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"Name\" column - we won't be using that data to train the model\n",
    "training_data = training_data.drop('Name', axis=1)\n",
    "\n",
    "# Load the 'HomePlanet' and'\"Destination' columns\n",
    "home_planets = training_data.get('HomePlanet').unique().tolist()[:-1]\n",
    "destinations = training_data.get('Destination').unique().tolist()[:-1]\n",
    "\n",
    "# Convert both columns to a dictionary \n",
    "home_planets_map = {k: v for v, k in enumerate(home_planets)}\n",
    "destinations_map = {k: v for v, k in enumerate(destinations)}\n",
    "\n",
    "# Turn both columns into their respective keys in the dictionary\n",
    "training_data['HomePlanet'] = training_data['HomePlanet'].replace(home_planets_map)\n",
    "training_data['Destination'] = training_data['Destination'].replace(destinations_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cabin IDs are a problem\n",
    "Cabin IDs are structured as follows: \"letter/number/letter\" (i.e B/0/P).\n",
    "\n",
    "We need to:\n",
    "- encode them\n",
    "- replace missing values in a non-random way\n",
    "\n",
    "The number in the middle seems to be increasing non-monotonically, so I decided to replace missing values for a random number in the following range: $|(\\text{prev valid value} - 5, \\text{prev valid value})|$.\n",
    "\n",
    "For the letters, it's clear that they repeat, so I decided to we create probability mappings and replace missing letters based on those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the 'Cabin' column, whose values are formatted as 'letter/number/letter'\n",
    "cabins = training_data.get('Cabin')\n",
    "\n",
    "# Separate 'Cabin' into 'Cabin_N' with N \\in {1,2,3}\n",
    "pattern = re.compile(r'^([a-zA-Z])/(\\d+)/([a-zA-Z])$')\n",
    "\n",
    "parsed_cabins = [\n",
    "    (matches.group(1), int(matches.group(2)), matches.group(3)) if (matches := pattern.match(str(cabin)))\n",
    "    else ('NA', -1, 'NA')\n",
    "    for cabin in cabins\n",
    "]\n",
    "\n",
    "# Transpose the parsed_cabins to separate lists for cabin_1, cabin_2, and cabin_3\n",
    "cabin_1, cabin_2, cabin_3 = map(list, zip(*parsed_cabins))\n",
    "\n",
    "# Replace missing numbers for a random value in |(previous valid value - 5, previous valid value)| \n",
    "# just because it seems to work\n",
    "prev_valid_val = None\n",
    "cabin_2 = [abs(random.randint(prev_valid_val-10, prev_valid_val)) if x == -1 else (prev_valid_val := x) for x in cabin_2]\n",
    "\n",
    "\n",
    "# Methods to compute probabilities for letter in the cabin IDs\n",
    "def list_to_probability_mapping(value_list):\n",
    "    value_counts = Counter(value_list)\n",
    "    total_count = len(value_list)\n",
    "\n",
    "    probability_mapping = {value: count / total_count for value, count in value_counts.items()}\n",
    "\n",
    "    return probability_mapping\n",
    "\n",
    "def get_index_from_probability_mapping(probability_mapping):\n",
    "    indices = list(range(len(probability_mapping)))\n",
    "    probabilities = list(probability_mapping.values())\n",
    "    return np.random.choice(indices, p=probabilities)\n",
    "\n",
    "def get_mean_val_randomized(dictionary):\n",
    "  return int(np.random.normal(loc=mean(dictionary), scale=2))\n",
    "\n",
    "# Create probability mappings\n",
    "cabin_1_map = list_to_probability_mapping(cabin_1)\n",
    "cabin_3_map = list_to_probability_mapping(cabin_3)\n",
    "\n",
    "# Replace missing letters for another value, weighing probability\n",
    "cabin_1 = [get_index_from_probability_mapping(cabin_1_map) if x == 'NA' else x for x in cabin_1]\n",
    "cabin_3 = [get_index_from_probability_mapping(cabin_3_map) if x == 'NA' else x for x in cabin_3]\n",
    "\n",
    "# Replace the 'Cabin' column for 'Cabin_ID_1, Cabin_ID_2, Cabin_ID_3' with their respective values\n",
    "training_data = training_data.drop('Cabin', axis=1)\n",
    "training_data['Cabin_ID_1'] = cabin_1\n",
    "training_data['Cabin_ID_2'] = cabin_2\n",
    "training_data['Cabin_ID_3'] = cabin_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning booleans into numbers is easier\n",
    "The \"CryoSleep\", \"VIP\" and \"Transported\" columns can be turned into numbers trivially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['CryoSleep'] = np.where(training_data['CryoSleep'] == True, 1, 0)\n",
    "training_data['VIP'] = np.where(training_data['VIP'] == True, 1, 0)\n",
    "training_data['Transported'] = np.where(training_data['Transported'] == True, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data (assuming \"Transported\" is the last column!!)\n",
    "X = training_data.iloc[:, 0:-1]\n",
    "y = training_data.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Save processed training data in a new CSV\n",
    "training_data.to_csv(base_file_path+'train_processed.csv', index=False)\n",
    "\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- Replacing cabin_1, cabin_3 for numerical data\n",
    "- Turning Passenger IDs into numbers\n",
    "- Group non-essential and essential spending categories together"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
